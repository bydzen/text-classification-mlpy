{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Deep Learning for Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lfu6XLZfrxJ"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hEPRXDqVfFQ"
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from keras.metrics import categorical_accuracy"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFM7XtD6fwnK"
      },
      "source": [
        "# Preprocessing the data\n",
        "You already learned that we have to tokenize the text before we can feed it into a neural network. This tokenization process will also remove some of the features of the original text, such as all punctuation or words that are less common."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLJjQkjRHvfd"
      },
      "source": [
        "# http://qwone.com/~jason/20Newsgroups/\n",
        "dataset = fetch_20newsgroups(subset='all', shuffle=True)\n",
        "\n",
        "texts = dataset.data # Extract text.\n",
        "target = dataset.target # Extract target."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSCS2FJq4GDy",
        "outputId": "59fea8d5-3fc6-41ce-c5a7-e9b214996072"
      },
      "source": [
        "dataset.target_names"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYpYyZ5iVqH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539e4d4e-9b5e-47fd-a7d0-33eec29f2ea8"
      },
      "source": [
        "print (target[:10])\n",
        "\n",
        "print (len(texts))\n",
        "print (len(target))\n",
        "print (len(texts[0].split()))\n",
        "print (texts[0])\n",
        "print (target[0])\n",
        "print (dataset.target_names[target[0]])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10  3 17  3  4 12  4 10 10 19]\n",
            "18846\n",
            "18846\n",
            "157\n",
            "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
            "Subject: Pens fans reactions\n",
            "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
            "Lines: 12\n",
            "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
            "\n",
            "\n",
            "\n",
            "I am sure some bashers of Pens fans are pretty confused about the lack\n",
            "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
            "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
            "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
            "are killing those Devils worse than I thought. Jagr just showed you why\n",
            "he is much better than his regular season stats. He is also a lot\n",
            "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
            "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
            "regular season game.          PENS RULE!!!\n",
            "\n",
            "\n",
            "10\n",
            "rec.sport.hockey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ymzR5_gSWp"
      },
      "source": [
        "Remember we have to specify the size of our vocabulary. Words that are less frequent will get removed. In this case we want to retain the 20,000 most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBTD2yxWWhuW"
      },
      "source": [
        "vocab_size = 20000 # Define the vocabulary size\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
        "tokenizer.fit_on_texts(texts) # Fitting the tokenizer on the data\n",
        "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntT3kdOiWkYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb9851b-53ee-4dd4-b04d-234a5f37ac2e"
      },
      "source": [
        "print (tokenizer.texts_to_sequences(['Hello King, how are you?']))\n",
        "\n",
        "print (len(sequences))\n",
        "print (len(sequences[0]))\n",
        "print (sequences[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1595, 1168, 82, 20, 13]]\n",
            "18846\n",
            "160\n",
            "[14, 19415, 455, 559, 15, 29, 2552, 1240, 5609, 33, 322, 767, 2175, 2121, 871, 1343, 32, 251, 88, 77, 84, 12087, 455, 559, 15, 7, 122, 228, 63, 3, 2552, 1240, 20, 517, 3490, 50, 1, 1393, 3, 61, 437, 3, 1507, 50, 1, 1302, 2552, 3027, 3, 1, 2701, 309, 7, 122, 243, 16334, 175, 5, 4, 243, 19416, 268, 7, 122, 194, 2, 296, 37, 337, 2, 369, 4389, 22, 4, 243, 3, 7286, 12, 1, 2552, 349, 30, 20, 1502, 137, 2701, 1382, 90, 7, 397, 5987, 74, 2025, 13, 130, 56, 8, 140, 215, 90, 93, 1457, 770, 1963, 56, 8, 97, 4, 308, 9186, 1857, 2, 1306, 6, 1, 2327, 6760, 115, 348, 5987, 21, 4, 308, 3, 1857, 6, 1, 365, 658, 3, 467, 185, 1, 2552, 20, 194, 2, 1985, 1, 66, 3, 3215, 608, 7, 26, 132, 8755, 19, 2, 131, 1, 3280, 2000, 1, 1151, 1457, 770, 283, 2552, 1222]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtrKOp1xWpix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f37e2e-70a7-4702-b96d-14c94de9b683"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {:,} unique words.'.format(len(word_index)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 179,209 unique words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykeKKvSbgY1Y"
      },
      "source": [
        "Our text is now converted to sequences of numbers. It makes sense to convert some of those sequences back into text to check what the tokenization did to our text. To this end we create an inverse index that maps numbers to words while the tokenizer maps words to numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2EmQGmYWrqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646988e3-26e2-465d-ed6a-b503c8449b72"
      },
      "source": [
        "# Create inverse index mapping numbers to words\n",
        "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "# Print out text again\n",
        "for w in sequences[0]:\n",
        "    x = inv_index.get(w)\n",
        "    print(x,end = ' ')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from ratnam andrew cmu edu subject pens fans reactions organization post office carnegie mellon pittsburgh pa lines 12 nntp posting host po4 andrew cmu edu i am sure some of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils actually i am bit puzzled too and a bit relieved however i am going to put an end to non relief with a bit of praise for the pens man they are killing those devils worse than i thought jagr just showed you why he is much better than his regular season stats he is also a lot fo fun to watch in the playoffs bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the out of jersey anyway i was very disappointed not to see the islanders lose the final regular season game pens rule "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBNHmDpBf0H_"
      },
      "source": [
        "# Measuring text length\n",
        "Let's ensure all sequences have the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ8W0xMhWtKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb137cc-9ddd-4d50-98dd-7adb9b2ddd19"
      },
      "source": [
        "# Get the average length of a text\n",
        "avg = sum(map(len, sequences)) / len(sequences)\n",
        "\n",
        "# Get the standard deviation of the sequence length\n",
        "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
        "\n",
        "avg,std"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(292.4769712405816, 666.9329063050876)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy156ffJgmgM"
      },
      "source": [
        "You can see, the average text is about 300 words long. However, the standard deviation is quite large which indicates that some texts are much much longer. If some user decided to write an epic novel in the newsgroup it would massively slow down training. So for speed purposes we will restrict sequence length to 100 words. You should try out some different sequence lengths and experiment with processing time and accuracy gains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdmeYEG9WyuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f47c0a3-bf00-41a6-90d4-02dd60894d6d"
      },
      "source": [
        "print(pad_sequences([[1,2,3]], maxlen=5))\n",
        "print(pad_sequences([[1,2,3,4,5,6]], maxlen=5))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 2 3]]\n",
            "[[2 3 4 5 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKsVc5t9W1Ek"
      },
      "source": [
        "max_length = 300 # Set the maximum length of the each data\n",
        "data = pad_sequences(sequences, maxlen=max_length) # Padding each data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8Nn9WRf5sV"
      },
      "source": [
        "# Turning labels into One-Hot encodings\n",
        "Labels can quickly be encoded into one-hot vectors with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrdJ_9CTW6aF"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "labels = to_categorical(np.asarray(target))\n",
        "print('Shape of data:', data.shape)\n",
        "print('Shape of labels:', labels.shape)\n",
        "\n",
        "print (target[0])\n",
        "print (labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4631NhFf7Z4"
      },
      "source": [
        "# Split dataset into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reS_GKUPW76h"
      },
      "source": [
        "train_size = int(len(data) * .8) # Set training data size\n",
        "xtrain = data[:train_size]\n",
        "ytrain = labels[:train_size]\n",
        "\n",
        "xtest = data[train_size:]\n",
        "ytest = labels[train_size:]\n",
        "\n",
        "xtest_texts = texts[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gYt3FSZ-NfA"
      },
      "source": [
        "print(len(xtrain))\n",
        "print(len(xtest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900biRGbgCLW"
      },
      "source": [
        "# Create Model (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xdQrWsmZ1hd"
      },
      "source": [
        "modelNN = Sequential()\n",
        "modelNN.add(Input(shape=(max_length,)))\n",
        "modelNN.add(Activation('relu'))\n",
        "modelNN.add(Dense(128, activation='sigmoid'))\n",
        "modelNN.add(Dense(64, activation='sigmoid'))\n",
        "modelNN.add(Dense(20, activation='softmax'))\n",
        "modelNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqupyH-9Z5ax"
      },
      "source": [
        "modelNN.compile(optimizer='adam',\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=[categorical_accuracy])\n",
        "\n",
        "histNN = modelNN.fit(xtrain,ytrain,validation_split=0.2,epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEvnTmrdgD2k"
      },
      "source": [
        "# Create Model (Simple RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGgTRLytXQ44"
      },
      "source": [
        "modelRNN = Sequential()\n",
        "modelRNN.add(Embedding(input_dim=vocab_size,\n",
        "                       output_dim=64,\n",
        "                       input_length=max_length,\n",
        "                       trainable=False))\n",
        "modelRNN.add(SimpleRNN(64))\n",
        "modelRNN.add(Dense(20, activation='softmax'))\n",
        "modelRNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAwFCrupXeVt"
      },
      "source": [
        "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
        "modelRNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
        "\n",
        "histRNN = modelRNN.fit(xtrain, ytrain, validation_split=0.2, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_3xMISOn4l9"
      },
      "source": [
        "# Create Model (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmp0dc3Uk3qt"
      },
      "source": [
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3C00Obhk53T"
      },
      "source": [
        "modelLSTM = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo0-6PWUlG9b"
      },
      "source": [
        "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
        "modelLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
        "\n",
        "histLSTM = modelLSTM.fit(xtrain, ytrain, validation_split=0.2, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRTkrMzggGKU"
      },
      "source": [
        "# Create Model (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56-7KPpSX0CA"
      },
      "source": [
        "modelCNN = Sequential()\n",
        "modelCNN.add(Embedding(input_dim=vocab_size,\n",
        "                       output_dim=64,\n",
        "                       input_length=max_length,\n",
        "                       trainable=False))\n",
        "modelCNN.add(ConvID(64, 2, activation='relu'))\n",
        "modelCNN.add(MaxPooling1D(2))\n",
        "modelCNN.add(ConvID(64, 2, activation='relu'))\n",
        "modelCNN.add(MaxPooling1D(2))\n",
        "modelCNN.add(ConvID(64, 2, activation='relu'))\n",
        "modelCNN.add(MaxPooling1D(2))\n",
        "\n",
        "model CNN.add(Flatten())\n",
        "\n",
        "modelCNN.add(Dense(64, activation='relu'))\n",
        "modelCNN.add(Dense(20, activation='softmax'))\n",
        "modelCNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OraTretaJYq"
      },
      "source": [
        "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
        "modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
        "\n",
        "histCNN = modelCNN.fit(xtrain, ytrain, validation_split=0.2, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNNntQmpgyFw"
      },
      "source": [
        "Our model achieves 66% accuracy on the validation set. Systems like these can be used to assign emails in customer support centers, suggest responses, or classify other forms of text like invoices which need to be assigned to an department. Let's take a look at how our model classified one of the texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEWX8oOgAABX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(histNN, \"categorical_accuracy\")\n",
        "plot_graphs(histNN, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26vrcx0PgJsN"
      },
      "source": [
        "# Example Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeTojVepe1yF"
      },
      "source": [
        "example = xtest[1000] # Get the tokens\n",
        "print (xtest_texts[1000])\n",
        "\n",
        "# Print tokens as text\n",
        "for w in example:\n",
        "    x = inv_index.get(w)\n",
        "    print(x,end = ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-zCjoxbaLrX"
      },
      "source": [
        "# Get prediction\n",
        "pred = modelCNN.predict(example.reshape(1,100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK6m8IUMdTZu"
      },
      "source": [
        "# Output predicted category\n",
        "target_names[np.argmax(pred)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}